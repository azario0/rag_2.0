{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azario0/rag_2.0/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrkht-sDNvQa"
      },
      "source": [
        "# Algeria 2.0\n",
        "\n",
        "## Advanced Python programming for RAG systems using Google Gemini\n",
        "\n",
        "### Presented by Benmalek Zohir\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdS9JhlGNvQd"
      },
      "source": [
        "# Documents preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OWzyVmYNvQe",
        "outputId": "4e8cace1-b748-4eb2-a8dc-39c4c84b40c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################################################\n",
            "Example of the splits :\n",
            "['\\nThe Earth', 'rth, our h', 'r home pla']\n",
            "#################################################\n",
            "length of the chunck :\n",
            "10\n",
            "#################################################\n",
            "Number of chuncks : \n",
            "49\n"
          ]
        }
      ],
      "source": [
        "def split_text_with_overlap(text, chunk_size, overlap_size):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap_size\n",
        "    return chunks\n",
        "\n",
        "text = '''\n",
        "The Earth, our home planet, is a dynamic and complex system.\n",
        "It's approximately 4.5 billion years old and is the only known planet to harbor life.\n",
        "Our planet is composed of several layers, including the crust, mantle, and core.\n",
        "The Earth's surface is constantly changing due to tectonic plate movement, volcanic eruptions, and erosion.\n",
        "'''\n",
        "\n",
        "result=split_text_with_overlap(text,10,3)\n",
        "print(\"#################################################\")\n",
        "print('Example of the splits :')\n",
        "print(result[:3])\n",
        "print(\"#################################################\")\n",
        "print('length of the chunck :')\n",
        "\n",
        "print(len(result[0]))\n",
        "\n",
        "print(\"#################################################\")\n",
        "print('Number of chuncks : ')\n",
        "print(len(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnn6ijf5NvQf"
      },
      "source": [
        "# Setting up the api key :\n",
        "You can get yours from :\n",
        "https://aistudio.google.com/app/apikey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzpEL5M0NvQg"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "# Configure the Gemini API\n",
        "genai.configure(api_key='YOUR_GEMINI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp1tUs7HNvQh"
      },
      "source": [
        "# Embedding models list of google generative ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9_KGooGNvQi",
        "outputId": "6ea19ae3-1429-4e5d-ac17-5bd15891512c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/embedding-001\n",
            "models/text-embedding-004\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'embedContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAZAvLEeNvQi"
      },
      "source": [
        "# Embedding our documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FktDG2lNvQj",
        "outputId": "d3e2cb7b-e59c-4201-dc1a-8b457ab1dcb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding for document 'Lion': [0.040143497, -0.019579103, -0.023438316, -0.03869... TRIMMED\n",
            "Embedding for document 'Cat': [0.03371215, -0.03692612, -0.04624668, -0.08384791... TRIMMED\n",
            "Embedding for document 'Dog': [0.03929852, -0.033923797, -0.029821463, -0.022883... TRIMMED\n",
            "Embedding for document 'Daulphin': [0.048549414, -0.032747224, -0.025847968, -0.03428... TRIMMED\n",
            "Embedding for document 'Shark': [0.043408774, -0.06344833, -0.01202401, -0.0315866... TRIMMED\n",
            "Embedding for document 'Horse': [0.0063757957, -0.059231978, -0.057385504, -0.0299... TRIMMED\n",
            "Embedding for document 'Cow': [0.06828845, -0.07650595, -0.036208507, -0.044902,... TRIMMED\n",
            "Embedding for document 'Bird': [-0.0063274074, -0.06696137, -0.047402818, 0.01119... TRIMMED\n",
            "Embedding for document 'Fish': [0.0086052045, -0.03786592, -0.013129386, -0.02560... TRIMMED\n",
            "Number of embedded documents: 9\n",
            "Embedding dimension: 768\n"
          ]
        }
      ],
      "source": [
        "def embed_documents(documents):\n",
        "    embeddings = []\n",
        "    for doc in documents:\n",
        "        embedding_result = genai.embed_content(\n",
        "            model='models/embedding-001',\n",
        "            content=doc\n",
        "        )\n",
        "\n",
        "        # Extract the 'embedding' from the result\n",
        "        if 'embedding' in embedding_result:\n",
        "            embedding_vector = embedding_result['embedding']\n",
        "            print(f\"Embedding for document '{doc}': {str(embedding_vector)[:50]}... TRIMMED\")\n",
        "            embeddings.append(embedding_vector)\n",
        "        else:\n",
        "            print(f\"No 'embedding' field found for document: {doc}\")\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "documents = [\n",
        "    \"Lion\",\n",
        "    \"Cat\",\n",
        "    \"Dog\",\n",
        "    \"Daulphin\",\n",
        "    \"Shark\",\n",
        "    \"Horse\",\n",
        "    \"Cow\",\n",
        "    \"Bird\",\n",
        "    \"Fish\"\n",
        "]\n",
        "\n",
        "\n",
        "embedded_docs = embed_documents(documents)\n",
        "print(f\"Number of embedded documents: {len(embedded_docs)}\")\n",
        "\n",
        "# Assuming embeddings are vectors, check the length of the first embedding\n",
        "if embedded_docs and isinstance(embedded_docs[0], list):\n",
        "    print(f\"Embedding dimension: {len(embedded_docs[0])}\")\n",
        "else:\n",
        "    print(\"The embedding is not in a list-like structure.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv9x92cvNvQl"
      },
      "source": [
        "# Saving the embedded documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmR4zhMCNvQl",
        "outputId": "7b4aeb78-8d4a-4a4b-d2b1-9e06810e667e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG system saved in rag_system\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "embedding_dim = len(embedded_docs[0])\n",
        "embedded_docs_np = np.array(embedded_docs).astype('float32')\n",
        "\n",
        "# Normalize vectors for cosine similarity\n",
        "faiss.normalize_L2(embedded_docs_np)\n",
        "\n",
        "# Create and add vectors to FAISS index\n",
        "index = faiss.IndexFlatIP(embedding_dim)\n",
        "index.add(embedded_docs_np)\n",
        "\n",
        "# Create save directory\n",
        "save_folder = \"rag_system\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "# 1. Save FAISS index\n",
        "index_path = os.path.join(save_folder, \"index.faiss\")\n",
        "faiss.write_index(index, index_path)\n",
        "\n",
        "# 2. Save documents and their metadata\n",
        "doc_ids=[i for i in range(9)]\n",
        "docs_mapping = {\n",
        "    str(i): {\n",
        "        \"text\": doc,\n",
        "        \"doc_id\": doc_id,\n",
        "    }\n",
        "    for i, (doc, doc_id) in enumerate(zip(documents, doc_ids))\n",
        "}\n",
        "\n",
        "with open(os.path.join(save_folder, \"documents.json\"), \"w\") as f:\n",
        "    json.dump(docs_mapping, f)\n",
        "\n",
        "\n",
        "print(f\"RAG system saved in {save_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu_-pCAjNvQm"
      },
      "source": [
        "# Loading the embedded documents and the treated documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09Xayy4UNvQn",
        "outputId": "d31ee9e2-274d-4b3c-9b4e-29cbad058cc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Lion', 'Cat', 'Dog', 'Daulphin', 'Shark', 'Horse', 'Cow', 'Bird', 'Fish']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import faiss\n",
        "\n",
        "def load_rag_system(folder_path):\n",
        "    # Load FAISS index\n",
        "    index = faiss.read_index(os.path.join(folder_path, \"index.faiss\"))\n",
        "\n",
        "    # Load documents (using JSON in this example)\n",
        "    with open(os.path.join(folder_path, \"documents.json\"), \"r\") as f:\n",
        "        documents = json.load(f)\n",
        "\n",
        "    return index, documents\n",
        "\n",
        "index_path = \"rag_system\"\n",
        "index , documents_json= load_rag_system(index_path)\n",
        "\n",
        "sorted_data = sorted(documents_json.values(), key=lambda x: x['doc_id'])\n",
        "\n",
        "# Extract the 'text' values from the sorted data\n",
        "documents = [item['text'] for item in sorted_data]\n",
        "\n",
        "documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STPmxpCnNvQn"
      },
      "source": [
        "# Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jn715PWNvQn",
        "outputId": "02495240-9369-407c-c482-ebe1356c7a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the vectors\n",
        "vector_a = np.array([1, 0, 0])\n",
        "vector_b = np.array([0, 1, 1])\n",
        "\n",
        "# vector_a = np.array([1, 0, 0])\n",
        "# vector_b = np.array([-1, 0, 0])\n",
        "\n",
        "# Compute the dot product and magnitudes\n",
        "dot_product = np.dot(vector_a, vector_b)\n",
        "magnitude_a = np.linalg.norm(vector_a)\n",
        "magnitude_b = np.linalg.norm(vector_b)\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_similarity = dot_product / (magnitude_a * magnitude_b)\n",
        "\n",
        "print(f\"Cosine Similarity: {cosine_similarity}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swhjhm0bNvQo"
      },
      "source": [
        "# Retrieval :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5xqIGHPNvQo",
        "outputId": "63ced5bf-c860-46cc-df35-af72c6c2e071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Fish', 'Shark', 'Cat']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "def embed_text(text):\n",
        "    \"\"\"Embed text using Gemini's embedding-001 model.\"\"\"\n",
        "    embedding_result = genai.embed_content(\n",
        "            model='models/embedding-001',\n",
        "            content=text,\n",
        "            task_type='retrieval_query'\n",
        "        )\n",
        "    return embedding_result\n",
        "\n",
        "\n",
        "\n",
        "def retriever(query, k=3):\n",
        "    embedding_result = embed_text(query)\n",
        "    query_embedding = np.array(embedding_result['embedding']).astype('float32').reshape(1, -1)\n",
        "\n",
        "    # Normalize the query vector\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    retrieved_docs = [documents[i] for i in indices[0]]\n",
        "    return retrieved_docs\n",
        "\n",
        "\n",
        "# Your query\n",
        "query = \"lives under water\"\n",
        "retriever(query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4nA99iCNvQo"
      },
      "source": [
        "# The list of gemini models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjpNsHoNNvQp",
        "outputId": "20e83ffc-0928-4a79-8c52-f175a9163f67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-exp-0801\n",
            "models/gemini-1.5-pro-exp-0827\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-exp-0827\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'gemini' in m.name:\n",
        "    print(m.name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6EIVSnQNvQp"
      },
      "source": [
        "# Putting it all together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YKL9L-fNvQp",
        "outputId": "aca777f5-f189-4082-f462-82706cd01351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: what are the companion animals ?\n",
            "Response: The provided context (\"Dog Horse Cat\") offers a limited set of potential companion animals.  Determining which are definitively companion animals requires a nuanced understanding of the term.  While all three – dogs, horses, and cats – can form bonds with humans, the degree and nature of the companionship varies considerably, influenced by factors including breed, individual animal temperament, and the human-animal interaction.\n",
            "\n",
            "Dogs are widely considered quintessential companion animals.  Extensive research documents the long history of human-dog co-evolution and the strong social bonds they form (Serpell, 2009).  Their capacity for bidirectional communication, trainability, and demonstrated emotional responsiveness contribute to their role as companions.  This is supported by numerous studies illustrating the positive impacts of dog ownership on human physical and mental health (Barker et al., 2017).\n",
            "\n",
            "Cats, similarly, have a long history of cohabitation with humans, though their companionship often manifests differently than that of dogs.  Their independent nature does not preclude the formation of close bonds, with studies showing evidence of attachment and social interaction between cats and their owners (Turner, 2000). However, the nature of this companionship may be less overtly demonstrative than in the dog-human dyad.\n",
            "\n",
            "Horses present a more complex case. While many individuals form strong bonds with their horses, regarding them as companions, the nature of this companionship differs significantly.  The relationship often revolves around shared activities such as riding, driving, or caring for the animal.  The level of intimacy and reciprocal emotional expression may not always reach the same level typically observed in the dog-human or cat-human relationships (McComb, 2014).  Furthermore, the practical requirements of horse ownership (space, financial resources, expertise) significantly impact the accessibility of this form of companionship compared to dogs or cats.\n",
            "\n",
            "In conclusion, based on the provided context and existing literature, dogs are unequivocally classified as companion animals due to their well-documented social bonds and mutual benefits within the human-animal relationship. Cats also qualify, albeit with a potentially less outwardly expressed companionship.  Horses, while capable of forming strong bonds with humans and often considered companions, exhibit a relationship distinct in nature and accessibility compared to dogs and cats.  Therefore, a definitive answer regarding the \"companion animals\" within the provided set depends on the operational definition of \"companion animal,\" and the level of intimacy and reciprocal interaction considered essential for such classification.\n",
            "\n",
            "\n",
            "**References:**\n",
            "\n",
            "* Barker, S. B., et al. (2017). Companion animals and human health: A review. *BMC Veterinary Research*, *13*(1), 1-12.\n",
            "* McComb, K. (2014). *Animal communication*. University of Chicago Press.\n",
            "* Serpell, J. (2009). *In the company of animals: A study of human-animal relationships*. Cambridge University Press.\n",
            "* Turner, D. C. (2000). *The domestic cat: The biology of its behaviour*. Cambridge University Press.\n",
            "\n",
            "\n",
            "**Note:**  This response requires access to scholarly databases to find and properly cite relevant research papers.  The references provided are examples of the types of sources that would support the arguments.  You would need to replace these with actual papers relevant to the specific aspects of human-animal relationships discussed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_response(query, retrieved_docs):\n",
        "    model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
        "\n",
        "\n",
        "    # Normal Prompt\n",
        "\n",
        "    prompt = f\"\"\"This is a RAG system given the following context and query, provide a brief response based on the retrieved documents :\n",
        "\n",
        "    Context:\n",
        "    {' '.join(retrieved_docs)}\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Response:\"\"\"\n",
        "\n",
        "\n",
        "    #     Simple Factual Prompt\n",
        "\n",
        "    # prompt = f\"\"\"Given the context and query, extract the most relevant facts:\n",
        "    # Context:\n",
        "    # {' '.join(retrieved_docs)}\n",
        "    # Query: {query}\n",
        "    # Provide a concise, factual response.\"\"\"\n",
        "\n",
        "\n",
        "    #     Detailed Explanatory Prompt\n",
        "\n",
        "    # prompt = f\"\"\"Analyze the provided context in relation to the query.\n",
        "    # Context:\n",
        "    # {' '.join(retrieved_docs)}\n",
        "    # Query: {query}\n",
        "    # Explain the key points comprehensively, providing depth and nuance to the answer.\"\"\"\n",
        "\n",
        "\n",
        "    #     Academic/Scholarly Prompt:\n",
        "\n",
        "    prompt = f\"\"\"Using the retrieved scholarly sources, construct an academic-style response:\n",
        "    Context:\n",
        "    {' '.join(retrieved_docs)}\n",
        "    Query: {query}\n",
        "    Structure your response with clear arguments, cite relevant information, and maintain an objective tone.\"\"\"\n",
        "\n",
        "\n",
        "    #     Conversational Prompt\n",
        "\n",
        "    # prompt = f\"\"\"Respond to the query as if you're having a friendly, informative conversation:\n",
        "    # Context:\n",
        "    # {' '.join(retrieved_docs)}\n",
        "    # Query: {query}\n",
        "    # Explain the answer in a warm, accessible manner that's easy to understand.\"\"\"\n",
        "\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "# Example usage\n",
        "query = \"what are the companion animals ?\"\n",
        "retrieved_docs = retriever(query)\n",
        "response = generate_response(query, retrieved_docs)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PtMF2-QNvQq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}